# Bio5488 SP2025: Assignment 10

### Proteogenomics

**Due Date:** Friday, 4/4, 11:59 PM   (Start early, some programs may take hours to run)
**Last Modified:** 3/24/2025

---
## Overview

The goal of this homework is to gain experience preparing a custom proteome reference database from genomic data, executing a mass spectrometry-based proteomics analysis pipeline, and critically evaluating the results. We will look for evidence of novel splicing events by searching for junction peptides that straddle two exons in publicly available CPTAC data.

Always remember to use an interactive job to run command-line tools (e.g., python, bowtie2, bcftools)
on the RIS. Once you submit an interactive job, you must activate your conda environment to use the
tools and dependencies within the docker image. 

```bash
$ bsub -Is -q workshop-interactive -G compute-workshop -a 'docker(takinwe1/bio5488:0.0)'
/bin/bash
$ conda activate bio5488
```
---
### Part 1: Download the Crux software suite.

1. Go to http://crux.ms/ and click Downloads
2. Select the download that matches your operating system
3. Agree to licensing terms and download release 4.3 of Crux
4. Unzip the download
5. The program we’ll be running from the command line is in the bin/ folder
   
	Mac users- you may run into permissions issues the first time you run this. If you do, close the pop-up that says you can’t run it (but don’t let it delete the crux file). Go to Security and Privacy -> General. Somewhere near the lock on the bottom left there should be a message saying something to the effect of “crux was stopped from running, click here to change that”. Click the lock, enter your password/touch id, and allow crux to run. 

---
### Part 2: Obtain the SwissProt reference proteome from UniProtKB

1. Go to https://www.uniprot.org/ and click Species Proteomes
2. Type “Homo sapiens” in the search bar and search
3. Select the UP000005640 proteome
4. Click View proteins (underneath Components)
5. Select the “Reviewed (SwissProt)” Status on the top left
6. Click download
7. Select FASTA (canonical & isoforms) and uncompressed. Download.

---
### Part 3: Create a peptide database index for the human SwissProt proteome

1. Open up a terminal, create a directory for the assignment, and create a peptide index

```bash
%  mkdir Assignment10
```

```bash
%  cd Assignment10
```

```bash
%  [[PATH_TO_CRUX_BIN]/crux tide-index -–overwrite T -missed-cleavages 2 --mods-spec K+229.162932 --nterm-peptide-mods-spec X+229.16293 [PATH_TO_PROTEOME]/UP000005640_9606.fasta human_swissprot_TMT11
```
This creates a peptide index called human_swissprot_TMT11

 --mods-spec K+229.162932 is a static TMT11 modification on lysines
          Use unimod.org (login as guest) to lookup masses of other PTMs if you’re curious
 
 	
 --nterm-peptide-mods-spec X+229.16293 is a static TMT11 mod on any residue on a peptide N-terminus.
 
 
--missed-cleavages 2 includes peptides with up to 2 missed cleavages by trypsin, e.g. PEPTIDERKPEPTIDEK
 
The digestion enzyme is set to Trypsin by default
 
The default min and max peptide lengths are 6 and 50, respectively.
 
--overwrite T avoids the program stopping if output already exists (happens if you made a mistake and want to rerun something).
 
Note how many unique peptides were generated (# targets). 
 
Let’s explore how the database size changes with variable modifications

Repeat the indexing but add up to 3 oxidized methionines per peptide:
 
```bash
%  [PATH_TO_CRUX_BIN]/crux tide-index -–overwrite T -missed-cleavages 2 --mods-spec K+229.162932 –mods-spec 3M+15.994915 --nterm-peptide-mods-spec X+229.16293 [PATH_TO_PROTEOME]/UP000005640_9606.fasta human_swissprot_TMT11_oxM
```
Note the number of peptides generated (“INFO: Created XXX peptides.”)
 
Now also add up to 3 phosphorylations per peptide:

 
```bash
%  [ [PATH_TO_CRUX_BIN]/crux tide-index -–overwrite T -missed-cleavages 2 --mods-spec K+229.162932 –mods-spec 3M+15.994915 –mods-spec 3STY+79.966331 --nterm-peptide-mods-spec X+229.16293 [PATH_TO_PROTEOME]/UP000005640_9606.fasta human_swissprot_TMT11_oxM_pSTY
```

**Question 1:**
How many unique peptides were generated for our database using no variable PTMs, up to 3 oxidized methionines, and 3 oxidized methionines + 3 phosphorylations of STYs?

**Question 2:**
What identification results might you find if you forgot to add the TMT modifications to the database


 ---
 ### Part 4: Download raw mass spectrometry data from CPTAC’s lung adenocarcinoma study.
 
 Go to https://proteomic.datacommons.cancer.gov/pdc/
 
Click Lung Adenocarcinoma
 
We’ll only look at global proteome data, not phosphorylation enriched data, so find the row that says APOLLO-LUAD Proteome. Click on the 360 under the mzML header (mzML is the open standard data format).
 
   You can use the .raw files as well, but Crux only provides support to read .raw files on 
   windows.
 
Download the AP1_QEHF2_AP1_9_18.mzML.gz file
 
In the interest of time, we’ll only download 1 file, but in reality each sample is divided into multiple fractions (usually 25) and often analyzed in two technical replicates.
 
Unzip the file and stick into your assignment folder
 

---
### Part 5: Perform a database search (peptide identification pipeline only)
Use the terminal to run the following command:

```bash
% [PATH_TO_CRUX_BIN]/crux pipeline –overwrite T AP1_QEHF2_AP1_9_18.mzML human_swissprot_TMT11/
```

This will run a database search (Tide/Sequest), a peptide inference method (Percolator), and false discovery rate estimation.

The peptide identifications will be in crux-output/percolator.target.peptides.txt


---
### Part 6: Create a 6-frame translation of the human genome and perform a database search

Easy option: download the 6-frame translated FASTA file from box: (https://wustl.box.com/s/rttt8d2onn79ddx25nzkkpsu08v85olr)

 Do it yourself option (very slow, only if you need to do it for a project one day): download transcript data from RefSeq’s Consensus CDS project: 
 (https://ftp.ncbi.nlm.nih.gov/pub/CCDS/current_human/)
 
Grab the CCDS_nucleotide.20221027.fna.gz file

Use a Docker image for the emboss suite

```bash
% docker pull biocontainers/emboss:v6.6.0dfsg-7b1-deb_cv
```
```bash
% docker run --rm -it -v [PATH_TO_FOLDER_WITH_FASTA]/ data biocontainers/emboss:v6.6.0dfsg-7b1-deb_cv1 /bin/bash
```

Write a little script to call the following command on each entry in your fasta file and concatenate the result:
```bash
% sixpack -sequence my_sequence.fna
```

There are other tools out there, but most have GUIs.

Create a peptide index of your 6-frame translated database

```bash
%  [PATH_TO_CRUX_BIN]/crux tide-index –overwrite T --missed-cleavages 2 --mods-spec K+229.162932 --nterm-peptide-mods-spec X+229.16293 [PATH_TO_PROTEOME] human_CCDS_6frame_TMT11
```

Database search

This will overwrite your previous results, so either create a new directory or move the previous results somewhere.

```bash
% [PATH_TO_CRUX_BIN]/crux pipeline –overwrite T AP1_QEHF2_AP1_9_18.mzML human_ CCDS_6frame_MT11/
```


**Question 3:**
How much larger is the 6-frame proteome in terms of peptides generated by tide-index?

**Question 4:** 
How many peptides were ID’ed at a 5% FDR using both methods? Why did we ID fewer peptides with a bigger database?

**Question 5:**
List the out-of-frame peptides identified at a 5% FDR (percolator q-value). You can tell they’re not in the expected frame if the none of the IDs in the in “protein id” column end in “true:0”. “true” refers to it being the strand given in the nucleotide fasta file, and 0 referring to the translation start index (out of frame sequences would start somewhere else). BLAST them (https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastp&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome) against the human proteome SwissProt and determine if there is an alternative in-frame explanation for each peptide.


### What to turn in:

A document containing the answers to each of the questions. The percolator.target.peptides.txt file from your 6-frame database search. The pipeline.log.txt file from each of your database searches.
